# CLAUDE.md — Summer Hiking Series

## Project Purpose

Weekend day hiking planner for the Seattle metro area. Covers spring equinox through fall (40 weekends). Focused on curated, real-world trail data with trip reports.

## Architecture

The system is a data pipeline with four distinct modules:

```
scrapers/ → CSV files → db/ (SQLite) → ui/ (static HTML)
```

### 1. Scrapers (`scrapers/`)
- Each scraper targets a specific data source (WTA, AllTrails, etc.)
- Output: CSV files written to `data/raw/`
- Two scraper types:
  - **Hike info**: trail metadata (name, distance, elevation, difficulty, pass, location)
  - **Trip reports**: recent conditions, snow, trail status, user comments
- Scrapers are independent; each can be run on its own schedule

### 2. Data Layer (`data/`)
- `data/raw/` — raw CSV output from scrapers (never edited by hand)
- `data/processed/` — cleaned/merged CSVs ready for DB import
- CSVs use a stable schema with a `source` and `scraped_at` column for traceability

### 3. DB Module (`db/`)
- Manages a local SQLite database (`hiking.db`)
- Responsibilities:
  - Import CSVs into the database
  - Conflict resolution (newer record wins by default; configurable per field)
  - Schema management via versioned migration files (`db/migrations/`)
  - CLI entry point: `db/manage.py`
- Schema versions are tracked in a `schema_version` table

### 4. UI Module (`ui/`)
- Reads from `hiking.db` and generates static HTML
- Output written to `ui/dist/`
- Entry point: `ui/build.py`
- No external JS framework; plain HTML/CSS with minimal JS
- `index.html` at root is the generated artifact (not hand-edited)

## Conventions

- **Python** for all scripts (3.11+)
- **SQLite** for the database (single file, no server)
- **CSV** as the interchange format between scrapers and DB
- All scripts runnable standalone: `python scrapers/wta.py`, `python db/manage.py import`, etc.
- No ORM — raw SQL via `sqlite3` stdlib module
- Dependencies tracked in `requirements.txt`

## Key Files

| Path | Purpose |
|------|---------|
| `CLAUDE.md` | This file |
| `TASKS.md` | Active task tracker |
| `hiking.db` | SQLite database (generated, not committed) |
| `data/raw/` | Raw scraper output CSVs |
| `data/processed/` | Cleaned CSVs ready for import |
| `db/manage.py` | DB import, migration, and schema CLI |
| `db/migrations/` | Versioned SQL migration files |
| `ui/build.py` | HTML generation from DB |
| `ui/dist/` | Generated static site output |
| `scrapers/wta.py` | WTA hike info scraper |
| `scrapers/wta_reports.py` | WTA trip report scraper |

## Data Model (current schema)

**hikes**
- `id`, `trail_name`, `location`, `distance_miles`, `elevation_gain_ft`
- `difficulty`, `drive_time_min`, `season_window`, `required_pass`
- `highlight`, `wta_url`, `alltrails_url`
- `source`, `scraped_at`, `updated_at`

**trip_reports**
- `id`, `hike_id` (FK), `report_date`, `conditions`, `snow_level`
- `author`, `text_summary`, `source`, `scraped_at`

**schedule**
- `id`, `weekend_date`, `hike_id` (FK), `season`, `notes`

## What NOT to do

- Do not hand-edit files in `data/raw/` — those are scraper outputs
- Do not edit `index.html` directly — it is generated by `ui/build.py`
- Do not commit `hiking.db` — it is a build artifact
- Do not add ORMs or heavy frameworks; keep deps minimal
